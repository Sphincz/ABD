{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"background-color: transparent;\">\n",
    "<tr>\n",
    "<td><img src=\"spark-logo-trademark.png\" width=\"250px\" /></td>\n",
    "<td> + </td>\n",
    "<td><img src=\"Python_logo-768x325.png\" width=\"350px\" /></td>\n",
    "<td> + </td>\n",
    "<td><img src=\"logo-gray.png\" width=\"250px\" /></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark&nbsp;Tutorial\n",
    "\n",
    "Este tutorial é uma breve introdução sobre como usar o [Apache Spark](http://spark.apache.org/) usando um notebook, e alternativamente, lançando um processo num cluster. Este tutorial foi adaptado de um notebook disponível na [_community cloud_](https://community.cloud.databricks.com) da [DataBricks](https://databricks.com).\n",
    "\n",
    "O Spark dispõe de implementações eficientes de um conjunto de transformações e acções, que podem ser compostas por forma a processar e analisar dados. O Spark é exímio a distribuir estas operações num cluster, abstraíndo todos os detalhes de implementação inerentes. O Spark foi desenhado a pensar na escalabilidade e eficiência. Com o Spark, é possível começar a desenvolver uma solução num portátil, com um pequeno dataset, e depois usar o mesmo código para processar terabytes ou mesmo petabytes distribuídos ao longo de um cluster.\n",
    "\n",
    "** Este tutorial é composto pelas seguintes partes:**\n",
    "\n",
    "* [*Parte 1:* O que é o Apache Spark?](#Parte&nbsp;1:&nbsp;O&nbsp;que&nbsp;é&nbsp;o&nbsp;Apache&nbsp;Spark?)\n",
    "\n",
    "* [*Parte 2:* Core Spark Concepts](#Parte&nbsp;2:&nbsp;Core&nbsp;Spark&nbsp;Concepts) \n",
    "\n",
    "* [*Parte 3:* Uso de DataFrames e encadeamento de transformações e acções](#Parte&nbsp;3:&nbsp;Uso&nbsp;de&nbsp;DataFrames&nbsp;e&nbsp;encadeamento&nbsp;de&nbsp;transformações&nbsp;e&nbsp;acções)\n",
    "\n",
    "* [*Parte 4*: Funções Lambda e User Defined Functions](#Parte&nbsp;4:&nbsp;Funções&nbsp;Lambda&nbsp;e&nbsp;User&nbsp;Defined&nbsp;Functions)\n",
    "* [*Parte 5:* Acções adicionais sobre DataFrames](#Parte&nbsp;5:&nbsp;Acções&nbsp;adicionais&nbsp;sobre&nbsp;DataFrames)\n",
    "* [*Parte 6:* Transformações adicionais sobre DataFrames](#Parte&nbsp;6:&nbsp;Transformações&nbsp;adicionais&nbsp;sobre&nbsp;DataFrames)\n",
    "* [*Parte 7:* Caching DataFrames e opções de storage](#Parte&nbsp;7:&nbsp;Caching&nbsp;DataFrames&nbsp;e&nbsp;opções&nbsp;de&nbsp;storage)\n",
    "\n",
    "Vão ser abordadas as seguintes **transformações**:\n",
    "* `select()`, `filter()`, `distinct()`, `dropDuplicates()`, `orderBy()`, `groupBy()`\n",
    "\n",
    "Vão ser abordadas as seguintes **acções**:\n",
    "* `first()`, `take()`, `count()`, `collect()`, `show()`\n",
    "\n",
    "Igualmente importante são as seguintes operações:\n",
    "* `cache()`, `unpersist()`\n",
    "\n",
    "**Nota**: os detalhes de cada um destes métodos podem ser consultados em [Spark's PySpark SQL API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte&nbsp;1:&nbsp;O&nbsp;que&nbsp;é&nbsp;o&nbsp;Apache&nbsp;Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Apache Spark é uma plataforma de computação em _cluster_ desenhada para ser rápida e de uso geral. Do lado da velocidade, extende o popular MapReduce para incluir mais tipos de computação, incluíndo _queries_ interactivas e processamento de _streams_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Stack\n",
    "\n",
    "O Spark inclui múltiplos componentes, representados na figura seguinte:\n",
    "\n",
    "<img src=\"spark_stack.png\" width=\"450px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Spark Core contém a funcionalidade básica do Spark, incluindo componentes para agendamento de tarefas, gerenciamento de memória, recuperação de falhas, interação com sistemas de armazenamento e muito mais. O Spark Core contém também a API que define os _Resilient distributed datasets_ (RDDs), que são a principal abstração de programação do Spark. Os RDDs representam uma coleção de itens distribuídos por vários nós de computação num _cluster_, que podem ser manipulados em paralelo. O Spark Core fornece ainda outras APIs para a construção e manipulação dessas coleções."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL é o pacote do Spark desenhado para trabalhar com dados estruturados. Permite a consulta de dados via SQL, bem como a variante Apache Hive do SQL - chamada Hive Query Langage (HQL) - e suporta muitas fontes de dados, incluindo tabelas Hive, Parquet e JSON. Além de fornecer uma interface SQL para o Spark, o Spark SQL permite aos programadores juntar, numa mesma aplicação, consultas SQL com as manipulações de dados suportadas por RDDs em Python, Java e Scala. O Spark permite assim combinar o SQL com análises de dados mais complexas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Spark Streaming é um componente Spark que permite o processamento de _streams_ (fluxos de dados) em tempo real. Como exemplos de _streams_ incluem-se os ficheiros de log gerados por servidores Web em produção ou filas de mensagens contendo actualizações de _status_ geradas pelos utilizadores de redes sociais. \n",
    "\n",
    "O Spark Streaming disponibiliza uma API para manipular _streams_ que se aproximam da API RDD do Spark Core. Isso facilita a aprendizagem e permite aos programadores moverem-se entre aplicações que manipulam dados armazenados na memória, em disco ou chegando em tempo real. Na base da API, o Spark Streaming foi projetado para fornecer o mesmo grau de tolerância a falhas, _throughput_ e escalabilidade como Spark Core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Spark vem com uma biblioteca que contém as funcionalidades comuns de _machine learning_  chamada MLlib. A API MLlib fornece vários tipos de algoritmos de aprendizagem automática, incluindo classificação, regressão, agrupamento e filtragem colaborativa, além de suportar funcionalidades como avaliação de modelos e importação de dados. A API fornece também algumas primitivas de ML mais básicas, como um algoritmo genérico de optimização (_gradient descent_). Todos esses métodos foram desenhados para escalar ao longo de um _cluster_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphX é a biblioteca que permite manipular grafos (por exemplo, um grafo de uma rede social) e executar computações paralelas em grafos. Tal como o Spark Streaming ou o Spark SQL, o GraphX estende a API Spark RDD, permitindo criar um grafo dirigido com propriedades genéricas em cada vértice ou aresta. O GraphX dispõe também de várias operações para manipular grafos (por exemplo, _subgraph_ e _mapVertices_) e uma biblioteca com algoritmos de grafos comuns (por exemplo, PageRank e contagem de triângulos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Managers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na base, o Spark foi projetado para escalar eficientemente de um para muitos milhares de nós de computação. Para conseguir isso, ao mesmo tempo que se maximiza a flexibilidade, o Spark pode ser executado em vários _cluster managers_, incluindo o Hadoop YARN, o Apache Mesos e um _cluster manager_ simples incluído no próprio Spark, chamado _Standalone Scheduler_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Voltar ao topo](#Spark&nbsp;Tutorial)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte&nbsp;2:&nbsp;Core&nbsp;Spark&nbsp;Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada aplicação Spark consiste num _driver program_ que lança várias operações em paralelo distribuídas num _cluster_. O _driver program_ contém a função _main_ da aplicação, define como o dataset é distribuído no _cluster_, e depois aplica-lhes as operações. \n",
    "\n",
    "Para correr estas operações, o _driver program_ gere um conjunto de nós chamados _executors_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O _driver program_ acede ao Spark através de um objecto `SparkContext`, que representa a ligação a um _cluster_ computacional. \n",
    "\n",
    "<img src=\"driver_executor.png\" width=\"300px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após existir um objecto `SparkContext`, é possível usá-lo para criar RDDs, como se verá mais à frente neste tutorial.\n",
    "\n",
    "No Spark a comunicação ocorre entre o _driver_ e os _executors_. O _driver_ tem um conjunto de tarefas que quer correr, tarefas estas que são divididas em _task_ e submetidas aos _executors_ para as completarem. O resultado de todas estas _tasks_ são devolvidas ao _driver_.\n",
    "\n",
    "Por forma a usar o Spark e a sua API DataFrame é necessário usar o módulo `SQLContext`. Primeiro cria-se uma nova aplicação Spark criando um [SparkContext](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext). Depois cria-se um [SQLContext](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext) a partir do `SparkContext`. Quando o `SparkContext` é criado, ele pede para usar alguns _cores_ ao nó _master_. O nó _master_, em função da disponibilidade, aloca-lhe recursos que não serão usados por outra aplicação.\n",
    "\n",
    "Quando se usa o _PySpark_ são criado automaticamente um `SparkContext` e um `SQLContext`. `sc` é a variável `SparkContext`, e `sqlContext` a variável `SQLContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fe3dd539908>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) Exemplo de Cluster\n",
    "\n",
    "O diagrama em baixo mostra o exemplo de um _cluster_, onde os _slots_ alocados a uma aplicação estão representados a roxo. (Nota: o termo _slots_ é usado par indicar _threads_ disponívels para realizar tarefas paralelas para o Spark. A documentação refere estas _threads_ como _cores_, o que é confuso porque o número de _slots_ disponíveis numa máquina em particular não tem necessariamente que ver com o número de CPUs físicos dessa máquina.)\n",
    "\n",
    "\n",
    "<img src=\"diagram-2a.png\" style=\"height: 700px;\"/>\n",
    "\n",
    "Os detalhes da execução da aplicação Spark podem ser consultados na Spark web UI. A web UI está acessível no endereço: [https://iscte.me/clusters/](https://iscte.me/clusters/). No _tab_ \"Jobs\", é possível ver a lista de tarefas que foram agendadas para correr. É possível que ainda não haja nada de interessante nesta página, mas assim que começarem a serem executadas tarefas é posssível ver o seu estado.\n",
    "\n",
    "\n",
    "Em suma, cada aplicação Spark consiste de um _driver program_ que lança várias operações em paralelo nos _executors_ em máquinas virtuais de Java (JVMs), que correm num _cluster_ ou localmente na mesma máquina. Quando correm localmente, o `PySpark` é o _driver program_. Em todos os casos, o _driver program_ contém o _loop_ principal do programa, cria os RDDs no _cluster_ e depois aplica-les as operações (transformações e acções) nesses datasets.\n",
    "O objecto Spark SQL context (`sqlContext`) é o ponto de ligação ao Spark DataFrame e às funcionalidades SQL. O `SQLContext` é usado para criar DataFrames, que permitem realizar operações nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) Atributos do SparkContext \n",
    "\n",
    "É possível usar a função [dir()](https://docs.python.org/2/library/functions.html?highlight=dir#dir) do Python para obter a lista de todos os atributos e métodos acessívels a partir do objecto `sqlContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lista dos atributos do objecto sqlContext\n",
    "dir(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Versão do Apache Spark\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Particularmente útil:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Endereço da página web UI da aplicação\n",
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** apenas é possível ter uma única aplicação `PySpark` a correr localmente em simultâneo!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2c) Obter ajuda (help)\n",
    "\n",
    "Alternativamente é possível usar a função [help()](https://docs.python.org/2/library/functions.html?highlight=help#help) do Python para obter uma lista, mais fácil de ler, com todos os atributos, incluíndo exemplos, que o objecto `sqlContext` tem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use help to obtain more detailed information\n",
    "help(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** fora do contexto do `pyspark`, o objecto `SQLContext` é creado a partir do objecto de mais baixo nível `SparkContext`, que é usado para criar os RDDs. Um RDD é a forma como o Spark realmente representa os dados internamente. Os DataFrames são também implementados em termos de RDDs. \n",
    "\n",
    "Embora seja possível interagir directamente com os RDDs, é preferível usar DataFrames. Os DataFrames são geralmente mais rápidos, e apresentam o mesmo comportamento independentemente da linguagem usada com o SPark (Python, R, Scala or Java)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2d) Importação de bibliotecas de Python\n",
    "\n",
    "É possível importar bibliotecas de Python usando o comando `import`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importar a biblioteca de expressões regulares\n",
    "import re\n",
    "m = re.search('(?<=abc)def', 'abcdef')\n",
    "m.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importar a biblioteca datetime\n",
    "import datetime\n",
    "print('This was last run on: {0}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Voltar ao topo](#Spark&nbsp;Tutorial)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte&nbsp;3:&nbsp;Uso&nbsp;de&nbsp;DataFrames&nbsp;e&nbsp;encadeamento&nbsp;de&nbsp;transformações&nbsp;e&nbsp;acções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O primeiro DataFrame\n",
    "\n",
    "A primeira coisa a fazer no Spark é criar um [DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame) base. Depois, pode aplicar-se uma ou mais transformações a este DataFrame. *Um DataFrame é imutável, e por isso, uma vez criado, não pode ser alterado.* Como resultado, cada transformação cria um novo DataFrame. Finalmente, pode ser aplicado um ou mais acções a este conjunto de DataFrames\n",
    "\n",
    "> Note que o Spark usa o que se chama de _lazy evaluation_, o que significa que as transformações não são de facto executadas até ocorrer uma acção!\n",
    "\n",
    "Neste _notebook_ são realidados alguns exercícios para compreender melhor os DataFrames:\n",
    "* Criar uma colecção de Python com 10,000 inteiros\n",
    "* Criar um DataFrame Spark a partir dessa colecção\n",
    "* Subtrair um a cada valor usando a função `map`\n",
    "* Realizar a acção `collect` para ver os resultados\n",
    "* Realizar a acção `count` para ver as contagens\n",
    "* Aplicar a transformação `filter` e ver os resultados com `collect`\n",
    "* Aprender sobre funções lambda\n",
    "* Explorar como a _lazy evalutation_ funciona e quais os desafios que coloca para realizar _debug_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um DataFrame consiste numa série de objectos do tipo `Row`; cada objecto `Row` tem um conjunto de colunas com nome. Um DataFrame é parecido com uma folha de cálculo, ou um DataFrame do Python Pandas.\n",
    "\n",
    "Genericamente, um DataFrame tem o que se chama um _schema_, que signitica que consiste num conjunto de colunas, cada uma com um _nome_ e um _tipo_ de dados. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3a) Criar uma colecção Python com 10,000 pessoas \n",
    "\n",
    "Nesta parte vamos uasar um módulo de Python chamado [fake-factory](https://pypi.python.org/pypi/fake-factory/0.5.1) para criar uma colecção de registos falsos de pessoas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from faker import Factory\n",
    "fake = Factory.create()\n",
    "fake.seed(4321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos usar esta _factory_ para criar uma colecção aleatória de registos de pessoas. Na próxima secção vamos transformar esta colecção num DataFrame. Vamos usar a classe Spark `Row` porque vai ajudar a definir o _schema_ do DataFrame. Existem outras formas de definir _schemas_; ver  a documentação Spark Programming Guide's [schema inference](http://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection) for more information. (Por exemplo, podia ter sido usado o Python `namedtuple`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cada entrada consiste de Apelido, Primeiro nome, ssn, profissão e idade (pelo menos 1 ano)\n",
    "from pyspark.sql import Row\n",
    "def fake_entry():\n",
    "  name = fake.name().split()\n",
    "  return (name[1], name[0], fake.ssn(), fake.job(), abs(2016 - fake.date_time().year) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Função auxiliar para chamar uma função repetidadente\n",
    "def repeat(times, func, *args, **kwargs):\n",
    "    for _ in range(times):\n",
    "        yield func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = list(repeat(10000, fake_entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variável `data` é apenas uma lista de Python normal, contendo tuplos. Vamos ver o primeiro registo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar o tamanho da lista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Distribuição dos dados e uso da colecção para criar um DataFrame\n",
    "\n",
    "\n",
    "No Spark, os datasets são representados como uma lista de entradas, a qual é partida em várias partições diferentes que são guardadas em diferentes máquinas. Cada partição guarda apenas um subconjunto único de entradas da lista. O Spark chama aos datasets que guarda Resilient Distributed Datasets\" (RDDs). Até mesmo os DataFrames são representados como RDDs, com meta-dados adicionais.\n",
    "\n",
    "Uma das características do Spark, comparado com outros _frameworks_ para anális de dados (por exemplo o Hadoop), é que ele guarda os dados na memória em vez de ser no disco. Isso permite às aplicações correrem de forma mais rápida. A figura seguinte ilustra a forma como o Spark divide a lista de dados em partições que são depois guardadas na memória dos _workers_.\n",
    "\n",
    "<img src=\"diagram-3b.png\" style=\"width: 900px;\"/>\n",
    "\n",
    "Para criar um DataFrame, vamos usar o `sqlContext.createDataFrame()`, e vamos passar o nosso array de dados como argumento desta função. O Spark vai criar um novo conjunto dados baseado nos dados que lhe são passados. Um DataFrame precisa de um _schema_, que é uma lista de colunas onde cada coluna tem um nome e um tipo. A nossa lista de dados tem elementos com diferentes tipos (strings e uma coluna de inteiros). O resto do _schema_ e o nome das colunas é o segunto argumento da função  `createDataFrame()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consultar o help da função `createDataFrame()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(sqlContext.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF = sqlContext.createDataFrame(data, ('last_name', 'first_name', 'ssn', 'occupation', 'age'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver qual o tipo do objecto que o `sqlContext.createDataFrame()` retorna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('type of dataDF: {0}'.format(type(dataDF)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver o _schema_ do DataFrame e alguns dos seus `rows`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível registar este recém criado DataFrame como uma tabela com nome (_named table_), usando o método `registerDataFrameAsTable()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(dataDF, 'dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que métodos é possível invocar neste DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(dataDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em quantas partições o DataFrame foi dividido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Nota sobre DataFrames e _queries_**\n",
    "\n",
    "Quando se usa um DataFrame ou o Spark SQL, está-se a construir um _query plan_. Cada transformação que se aplica a um DataFrame adiciona informação do _query plan_. Quando finalmente é chamada uma acção, que inicia uma execução de uma tarefa no Spark, acontecem várias coisas:\n",
    "\n",
    "1. O _Spark Catalyst optimizer_ analisa o conjunto de _queries_ a procura optimizá-lo. As optimizações incluem rearranjar e combinar `filter()` operações de forma a o conjunto ser mais eficiente, converter operações `Decimal` em operações mais eficientes `long integer`, etc... O resultado desta fase de optimização chama-se _optimized logical plan_.\n",
    "\n",
    "2. Após o _Catalyst_ ter obtido um _optimized logical plan_, ele constrói vários _physical plans_. Mais concretamente, ele implementa a _query_ em termos de operações de baixo nível envolvendo operações com RDDs.\n",
    "\n",
    "3. O _Catalyst_ escolhe qual dos _physical plan_ vai usar por intermédio de um _cost optimization_. Isto é, determina que _physical plan_ é mais eficiente e usa-o.\n",
    "\n",
    "4. Finalmente, uma vez determinado o plano de execução que envolve RDDs, o Spark executa-o.\n",
    "\n",
    "É possível ver qual o plano a usar usando a função `explain()` num DataFrame. Por defeito, a função `explain()` apenas mostra o _physical plan_ final; no entanto, se passarmos o argumento `True`, ele mostra todas as fases.\n",
    "\n",
    "(O texto incluído neste blog, embora antigo, é uma excelente forma de conhecer como o Catalyst optimiza as queries sobre DataFrames: [Deep Dive into Spark SQL's Catalyst Optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html).)\n",
    "\n",
    "Vamos acrescentar algumas transformações ao DataFrame e ver o _query plan_ no DataFrame resultante. O aspecto é complicado ao início. Ao longo do tempo o uso do `explain()` ajudará a compreender melhor as operações sobre DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDF = dataDF.distinct().select('*')\n",
    "newDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3c): Subtrair um a cada entrada usando o _select_\n",
    "\n",
    "Até agora, criámos um DataFrame distribuído dividido em várias partições, onde cada partição é armazenada numa única máquina no nosso _cluster_. Vejamos o que acontece quando fazemos uma operação básica no _dataset_. Muitas operações úteis de análise de dados podem ser especificadas como \"fazer algo a cada item do _dataset_\". Essas operações paralelas são convenientes porque cada item do _dataset_ pode ser processado individualmente: uma operação numa entrada não afeta as operações nas outras entradas. O Spark pode assim paralelizar a operação.\n",
    "\n",
    "Uma das operações sobre DataFrames mais comuns é a operação `select ()`, e funciona mais ou menos como uma instrução SQL SELECT: é possível selecionar colunas específicas e até usar a operação  `select ()` para criar novas colunas cujos valores foram obtidos a partir das colunas existentes. A operação `select ()`  pode assim ser usado para criar uma nova coluna cujos valores sejam os da coluna `age` subtraídos de uma unidade.\n",
    "\n",
    "A operação `Select ()` é uma _transformação_. Ela devolve um novo DataFrame que compreende o DataFrame anterior e a operação a adicionar à consulta (`o select`, neste caso). No entanto o Spark  * não *  executa nada no _cluster_. Ao transformar DataFrames, estamos na verdade a construir um _query plan_. Este será otimizado, implementado (em termos de RDDs) e executado por Spark _apenas_ quando invocarmos uma ação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transformar o dataDF através de uma transformação e mudar o nome à coluna criada '(age -1)'\n",
    "# Uma vez que o select é uma transformação, o Spark apenas faz uma lazy evaluation: não há\n",
    "# tarefas, ou tasks lançadas no cluster.\n",
    "subDF = dataDF.select('last_name', 'first_name', 'ssn', 'occupation', (dataDF.age - 1).alias('age'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver o aspecto do _query plan_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3d) Usar o  _collect_ para ver os resultados\n",
    "\n",
    "<img src=\"diagram-3d.png\" style=\"height:600px;\"/>\n",
    "\n",
    "\n",
    "Para ver o conteúdo da lista de elementos resultante, é necessário criar uma nova lista no _driver_ a partir dos dados distribuídos nos nós _executor_. Para isso, pode ser usado o método `collect ()` no novo DataFrame. A operação `Collect ()` é frequentemente usada após transformações para garantir que estamos apenas retornar uma quantidade *pequena* de dados para o _driver_. Faz-se isto porque os dados a retornar para o _driver_ devem caber na memória disponível do _driver_. Caso isso não aconteça, o programa _driver_ falhará (_crash_).\n",
    "\n",
    "O método `collect ()` é a primeira operação de acção que estamos a ver. As operações de acção fazem com que o Spark execute as operações de transformação (_lazy_) necessárias para calcular os valores a devolver. No nosso exemplo, isso significa que o Spark lançará tarefas para executar as operações `createDataFrame`,` select` e `collect`.\n",
    "\n",
    "No diagrama acima, o conjunto de dados encontra-se dividido em quatro partições, pelo que serão iniciadas quatro tarefas `collect ()`. Cada tarefa recolhe as entradas de uma partição e envia o resultado para o _driver_, que cria uma lista dos valores.\n",
    "\n",
    "Vamos executar a operação `collect()` no DataFrame `subDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect dos dados\n",
    "results = subDF.collect()\n",
    "print (results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma forma melhor de visualizar os dados é usar o método `show()`. Por defeito ele mostra 20 entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível não _truncar_ os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subDF.show(n=30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3e) Uso do _count_ para obter o total\n",
    "\n",
    "Uma das tarefas mais básicas é que podemos executar é o `count()`, que conta o número de elementos existentes num DataFrame (usando a acção `count()`). Uma vez que a operação `select()` criou um novo DataFrame com o mesmo número de elementos que o DataFrame inicial, é de esperar que o resultado do `count()` seja o mesmo para ambos.\n",
    "\n",
    "Atenção que o `count()` é uma acção: se não tivéssemos chamado a acção `collect()`, o Spark iria executar a as operações da transformação assim que executássemos o `count()`.\n",
    "\n",
    "Cada _tast_ conta o número de entradas na sua partição e envia o resultado ao SparkContext, que por sua vez soma todos os valores. A figura seguinte mostra o que aconteceria se corressemos o `count()` num dataset pequeno com apenas 4 partições.\n",
    "\n",
    "<img src=\"diagram-3e.png\" style=\"height:600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dataDF.count())\n",
    "print(subDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3f) Aplicar a transformação _filter_ e ver os resultados com o  _collect_\n",
    "\n",
    "Vamos agora criar um novo DataFrame que apenas contém as pessoas cujas idades são menores que 10. Para isso, vamos usar a transformação `filter()`. (Pode usar-se o `where()`, que é um alias para `filter()`, para quem preferir uma sintaxe mais SQL-like). O método `filter()` é uma transformação que cria um novo DataFrame a partir do DataFrame de entrada, que mantém apenas os valores que fazem _match_ à expressão do filtro.\n",
    "\n",
    "A figura seguinte mostra como o Spark executaria esta transformação, num dataset pequeno com 4 partições:\n",
    "\n",
    "\n",
    "<img src=\"diagram-3f.png\" style=\"height:600px;\"/>\n",
    "\n",
    "Para ver o resultado do filtro, os elementos cuja idade é menor que 10, é necessário criar uma nova lista no _driver_ a partir dos dados distribuídos. Pode ser usado o `collect()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filteredDF = subDF.filter(subDF.age < 10)\n",
    "filteredDF.show(truncate=False)\n",
    "filteredDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Há muitas crianças precoces...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Voltar ao topo](#Spark&nbsp;Tutorial)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte&nbsp;4:&nbsp;Funções&nbsp;Lambda&nbsp;e&nbsp;User&nbsp;Defined&nbsp;Functions\n",
    "\n",
    "Neste exemplo, em vez de definir em separado uma função para a transformação `filter()`, vamos usar uma função `lambda()` do Python e registá-la como uma Spark _User Defined Function_ (UDF). Uma UDF é tipo especial de função que pode ser usada numa _query_ a um DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "less_ten = udf(lambda s: s < 10, BooleanType())\n",
    "lambdaDF = subDF.filter(less_ten(subDF.age))\n",
    "lambdaDF.show()\n",
    "lambdaDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's collect the even values less than 10\n",
    "even = udf(lambda s: s % 2 == 0, BooleanType())\n",
    "evenDF = lambdaDF.filter(even(lambdaDF.age))\n",
    "evenDF.show()\n",
    "evenDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Voltar ao topo](#Spark&nbsp;Tutorial)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte&nbsp;5:&nbsp;Acções&nbsp;adicionais&nbsp;sobre&nbsp;DataFrames\n",
    "\n",
    "Let's investigate some additional actions:\n",
    "\n",
    "Vamos investigar algumas acções adicionais:\n",
    "\n",
    "* [first()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.first)\n",
    "* [take()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.take)\n",
    "\n",
    "Quando de tem um novo dataset, algo que se costuma fazer é ter uma ideia da informação disponível. No Spark, podemos usar acções como `first()`, `take()`, e `show()`. Note-se que no caso das acções `first()` e `take()`, os elementos devolvidos depende de como o DataFrame se encontra dividido em partições.\n",
    "\n",
    "Em vez de se usar a acção `collect()`, é preferícel usar a acção `take(n)`, que devolve os n primeiros elementos de um DataFrame. A acção `first()` devolve o primeiro elemento do DataFrame, que é equivalente a `take(1)[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"first: {0}\\n\".format(filteredDF.first()))\n",
    "\n",
    "print (\"Four of them: {0}\\n\".format(filteredDF.take(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Voltar ao topo](#Spark&nbsp;Tutorial)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte&nbsp;6:&nbsp;Transformações&nbsp;adicionais&nbsp;sobre&nbsp;DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6a) _orderBy_\n",
    "\n",
    "[`orderBy()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct) \n",
    "permite ordenar um DataFrame por uma ou mais colunas, resultando num novo DataFrame.\n",
    "\n",
    "\n",
    "Por exemplo, vamos obter as cinco primeiras pessoas mais velhas no DataFrame original (não filtrado). Podemos usar a transformação `orderBy()`, que recebe uma ou mais colunas, definidas como nomes (strings) or objectos `Column`. Para obter o objecto `Column`, usamos uma das seguintes notações:\n",
    "\n",
    "* notação ao estilo do Pandas: `filteredDF.age`\n",
    "* notação de _Subscript_: `filteredDF['age']`\n",
    "\n",
    "Ambas as sintaxes devolvem uma a `Column`, que têm métodos adicionais como `desc()` (para ordenar por ordem decrescente) or `asc()` (para ordenar por ordem crescente, que é o defeito)).\n",
    "\n",
    "Alguns exemplos:\n",
    "```\n",
    "dataDF.orderBy(dataDF['age'])  # sort by age in ascending order; returns a new DataFrame\n",
    "dataDF.orderBy(dataDF.last_name.desc()) # sort by last name in descending order\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the five oldest people in the list. To do that, sort by age in descending order.\n",
    "dataDF.orderBy(dataDF.age.desc()).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenar por ascendente (que é o defeito):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF.orderBy('age').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (6b) _distinct_ e _dropDuplicates_\n",
    "\n",
    "[`distinct()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct)\n",
    "filtra os _rows_ duplicados, e considera todas as colunas. Como os nossos dados são completamente aleatórios, é pouco provavel que haja duplicados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (dataDF.count())\n",
    "print (dataDF.distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Para demonstrar o `distinct()`, vamos criar um pequeno dataset de exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tempDF = sqlContext.createDataFrame([(\"Joe\", 1), (\"Joe\", 1), (\"Anna\", 15), (\"Anna\", 12), (\"Ravi\", 5)], ('name', 'score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tempDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tempDF.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note-se que uma das entradas dos (\"Joe\", 1) foi apagada, mas ambas as entradas com o nome \"Anna\" foram preservadas, porque todas as colunas têm de fazer _match_ para serem consideradas um duplicado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[`dropDuplicates()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropDuplicates) é parecido com o  `distinct()`, mas permite especificar as colunas a comparar. Por exemplo, podemos apagar todas as entradas em que o primeiro e o último nome são duplicados (ignorando as colunas da profissão e a idade).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (dataDF.count())\n",
    "print (dataDF.dropDuplicates(['first_name', 'last_name']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (6c) _drop_\n",
    "\n",
    "[`drop()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.drop) é o oposto ao `select()`: em vez de seleccionar apenas colunas específicas de um DataFrame, ele apaga as respectivas colunas.\n",
    "\n",
    "Exemplo de utilização: suponhamos que estamos a ler um ficheiro CSV com 1,000 colunas, e que queremos apagar 5 colunas. Em vez de seleccionar 995 colunas, é mais simples apagar apenas as cinco que não queremos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF.drop('occupation').drop('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (6d) _groupBy_\n",
    "\n",
    "[`groupBy()`]((http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy) é umas das transformações mais potentes. Ela permite realizar agregações num DataFrame.\n",
    "\n",
    "Ao contrário de outras transformações, o `groupBy()` não devolve um DataFrame novo. Em vez disso, ele devolve um objecto especial  [GroupedData](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData) que contém várias funções de agregação.\n",
    "\n",
    "A função de agregação mais comum é a função [count()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.count), mas existem outras como a função  [sum()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.sum), [max()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.max), e [avg()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.avg) (média).\n",
    "\n",
    "Estas funções de agregação criam uma nova coluna e devolvem um DataFrame novo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF.groupBy('occupation').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF.groupBy().avg('age').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poremos usar o  `groupBy()` para realizar outro tipo de agregações úteis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Maximum age: {0}\".format(dataDF.groupBy().max('age').first()[0]))\n",
    "print (\"Minimum age: {0}\".format(dataDF.groupBy().min('age').first()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Voltar ao topo](#Spark&nbsp;Tutorial)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte&nbsp;7:&nbsp;Caching&nbsp;DataFrames&nbsp;e&nbsp;opções&nbsp;de&nbsp;storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7a) Caching DataFrames\n",
    "\n",
    "\n",
    "Por questões de eficiência, o Spark mantém os DataFrames na memória. (Mais formalmente, ele mantém os _RDDs_ que implementam os DataFrames na memória.) Ao manter o conteúdo na memória, o Spark acede aos dados rapidamente. No entanto, a memória é limitada. Assim, se tivermos muitas partições na memória, chega a uma altura em que o Spark precisará de libertar espaço para novas partições. Caso o programa precise de uma partição que foi excluída, o o Spark irá recriá-la automaticamente. No entanto isso vem um custo: demora tempo.\n",
    "\n",
    "Assim, se se prevê usar um DataFrame mais de uma vez, é possível pedir ao Spark para armazená-lo em cache. A operação `cache ()` faz isso mesmo: mantém um DataFrame na memória. No entanto, deve usar-se uma ação no DataFrame, como `collect ()` ou `count ()` antes de chamar o `cache()`. Isto porque o `cache ()` é uma operação _lazy_: ela apenas informa o Spark que o DataFrame deve ser armazenado em cache _quando os dados são materializados_ (ocorre uma acção). \n",
    "\n",
    "Os dados armazenados em cache podem ser consultados na secção \"Storage\" da web UI do Spark. \n",
    "Você pode ver seu DataFrame armazenado em cache na seção \"Armazenamento\" da IU da Web do Spark. Se você clicar no valor do nome, poderá ver mais informações sobre onde o DataFrame está armazenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cache the DataFrame\n",
    "filteredDF.cache()\n",
    "# Trigger an action\n",
    "print (filteredDF.count())\n",
    "# Check if it is cached\n",
    "print (filteredDF.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (7b) Unpersist e opções de storage\n",
    "\n",
    "O Spark gere automaticamente as partições armazenadas em cache na memória. Se ele tiver mais partições do que a memória disponível, por padrão, ele evita partições mais antigas para dar espaço às mais novas. Por razões de  eficiência, quando o DataFrame armazenado em cache já não for usado, é possivel dizer ao Spark para o libertar da memória cache usando o método `unpersist ()` do DataFrame.\n",
    "\n",
    "\n",
    "Spark automatically manages the partitions cached in memory. If it has more partitions than available memory, by default, it will evict older partitions to make room for new ones. For efficiency, once you are finished using cached DataFrame, you can optionally tell Spark to stop caching it in memory by using the DataFrame's `unpersist()` method to inform Spark that you no longer need the cached data.\n",
    "\n",
    "** Opções avançadas: ** O Spark oferece muito mais opções para gerir a forma como os DataFrames são armazenados em cache. Por exemplo, é possível dizer ao Spark para passar partições da cache para o disco, quando ele ficar sem memória, em vez de as descartar. Estas opções podem ser exploradas usando a operação [persist()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.persist). Para consultar as opções pode usar a função do Python's [help()](https://docs.python.org/2/library/functions.html?highlight=help#help).  Opcionalmente, a operação `persist()` aceita um objecto  [StorageLevel](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.StorageLevel) do PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we are done with the DataFrame we can unpersist it so that its memory can be reclaimed\n",
    "filteredDF.unpersist()\n",
    "# Check if it is cached\n",
    "print (filteredDF.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Voltar ao topo](#Spark&nbsp;Tutorial)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
