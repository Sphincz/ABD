{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"background-color: transparent;\">\n",
    "<tr>\n",
    "<td><img src=\"../tutorial/spark-logo-trademark.png\" width=\"250px\" /></td>\n",
    "<td> + </td>\n",
    "<td><img src=\"../tutorial/Python_logo-768x325.png\" width=\"350px\" /></td>\n",
    "<td> + </td>\n",
    "<td><img src=\"../tutorial/logo-gray.png\" width=\"250px\" /></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcular o valor numérico de $\\pi$ no cluster\n",
    "\n",
    "Quando escolhemos como _kernel_ do _notebook_ o `PySpark`, o código é executado localmente na máquina principal do cluster. Para executar nos diferentes nós, é necessário usar o comando `spark-submit`, e o código deve estar todo dentro de um único ficheiro `.py`.\n",
    "\n",
    "Ao ser executado o código deste ficheiro, a variável `sc` não está imediatamente disponível. Para isso é necessário acrescentar ao código as seguintes instruções:\n",
    "\n",
    "    import pyspark\n",
    "    conf = pyspark.SparkConf()\n",
    "    sc = pyspark.SparkContext(conf=conf)\n",
    "    \n",
    "A função  `SparkConf()` permite definir opções para a forma como o código é executado no cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar se existe o spark context\n",
    "\n",
    "**Recorde que no cluster apenas é possível um kernel PySpark activo a cada instante.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports e definição de funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo do PI\n",
    "\n",
    "A variável `NUM_SAMPLES` define o número de amostras no cálculo do $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 100000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
    "             .filter(inside).count()\n",
    "t1 = time.time()\n",
    "\n",
    "print (\"Pi é aproximadamente %f\" % (4.0 * count / NUM_SAMPLES))\n",
    "print (\"O cálculo demorou %.2fs localmente.\" % (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar um ficheiro python para correr no cluster\n",
    "\n",
    "**Recorde:** o _magic_ `%%file` cria um ficheiro com o conteúdo da célula do notebook. Assim, a próxima linha cria um ficheiro chamado compute_pi.py, com o conteúdo:\n",
    "\n",
    "    from scipy import random\n",
    "    import pyspark\n",
    "    ...\n",
    "    print (\"Pi é aproximadamente %f\" % (4.0 * count / NUM_SAMPLES))\n",
    "    print (\"O cálculo demorou %.2fs localmente.\" % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%file compute_pi.py\n",
    "from scipy import random\n",
    "import pyspark\n",
    "import time\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "NUM_SAMPLES = 100000000\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "t0 = time.time()\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
    "             .filter(inside).count()\n",
    "t1 = time.time()\n",
    "print (\"Pi é aproximadamente %f\" % (4.0 * count / NUM_SAMPLES))\n",
    "print (\"O cálculo demorou %.2fs.\" % (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificar o conteúdo do ficheiro `compute_pi.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat compute_pi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executar o ficheiro `compute_pi.py` no cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!/usr/share/spark/bin/spark-submit \\\n",
    "--master spark://192.168.1.105:7077 \\\n",
    "compute_pi.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
