{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo do cálculo da frequência de palavras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar que existe o SparkContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f4f75f97860>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolha do dataset: local ou no HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ficheiro local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#file = '/home/ABD/datasets/other/pg100.txt'\n",
    "file = '/home/ABD/datasets/ch6/wiki_01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 spark spark 524283134 May 19  2017 /home/ABD/datasets/ch6/wiki_01\r\n"
     ]
    }
   ],
   "source": [
    "#!ls -l '/home/ABD/datasets/other/pg100.txt'\n",
    "!ls -l '/home/ABD/datasets/ch6/wiki_01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ficheiro no HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_hdfs = \"hdfs://master:9000/home/ABD/datasets/ch6/wiki_01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ler o ficheiro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text_file = sc.textFile(file_hdfs)\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contar a frequência de palavras localmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de palavras no ficheiro: 2482193\n",
      "O cálculo demorou 18.23s.\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "text_file = sc.textFile(file)\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "number_of_words = counts.count()\n",
    "t1 = time.time()\n",
    "print (\"Número de palavras no ficheiro: %d\" % (number_of_words))\n",
    "print (\"O cálculo demorou %.2fs.\" % (t1-t0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contar a frequência de palavras localmente (HDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de palavras no ficheiro: 2482193\n",
      "O cálculo demorou 30.86s.\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "text_file = sc.textFile(file_hdfs)\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "number_of_words = counts.count()\n",
    "t1 = time.time()\n",
    "print (\"Número de palavras no ficheiro: %d\" % (number_of_words))\n",
    "print (\"O cálculo demorou %.2fs.\" % (t1-t0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contar a frequência de palavras no cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting words_frequency.py\n"
     ]
    }
   ],
   "source": [
    "%%file words_frequency.py\n",
    "import pyspark\n",
    "import time\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "file_hdfs = \"hdfs://master:9000/home/ABD/datasets/ch6/wiki_01\"\n",
    "t0 = time.time()\n",
    "text_file = sc.textFile(file_hdfs)\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "number_of_words = counts.count()\n",
    "t1 = time.time()\n",
    "print (\"Número de palavras no ficheiro: %d\" % (number_of_words))\n",
    "print (\"O cálculo demorou %.2fs.\" % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pyspark\r\n",
      "import time\r\n",
      "\r\n",
      "conf = pyspark.SparkConf()\r\n",
      "sc = pyspark.SparkContext(conf=conf)\r\n",
      "file_hdfs = \"hdfs://master:9000/home/ABD/datasets/ch6/wiki_01\"\r\n",
      "t0 = time.time()\r\n",
      "text_file = sc.textFile(file_hdfs)\r\n",
      "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\r\n",
      "             .map(lambda word: (word, 1)) \\\r\n",
      "             .reduceByKey(lambda a, b: a + b)\r\n",
      "number_of_words = counts.count()\r\n",
      "t1 = time.time()\r\n",
      "print (\"Número de palavras no ficheiro: %d\" % (number_of_words))\r\n",
      "print (\"O cálculo demorou %.2fs.\" % (t1-t0))"
     ]
    }
   ],
   "source": [
    "!cat words_frequency.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "17/12/14 10:25:34 INFO SparkContext: Running Spark version 2.1.0\n",
      "17/12/14 10:25:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "17/12/14 10:25:35 INFO SecurityManager: Changing view acls to: joao\n",
      "17/12/14 10:25:35 INFO SecurityManager: Changing modify acls to: joao\n",
      "17/12/14 10:25:35 INFO SecurityManager: Changing view acls groups to: \n",
      "17/12/14 10:25:35 INFO SecurityManager: Changing modify acls groups to: \n",
      "17/12/14 10:25:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(joao); groups with view permissions: Set(); users  with modify permissions: Set(joao); groups with modify permissions: Set()\n",
      "17/12/14 10:25:35 INFO Utils: Successfully started service 'sparkDriver' on port 39487.\n",
      "17/12/14 10:25:35 INFO SparkEnv: Registering MapOutputTracker\n",
      "17/12/14 10:25:35 INFO SparkEnv: Registering BlockManagerMaster\n",
      "17/12/14 10:25:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "17/12/14 10:25:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "17/12/14 10:25:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dab3a1a5-1d5a-4ad5-8bc4-e21aa2091ff7\n",
      "17/12/14 10:25:35 INFO MemoryStore: MemoryStore started with capacity 3.0 GB\n",
      "17/12/14 10:25:35 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "17/12/14 10:25:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "17/12/14 10:25:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "17/12/14 10:25:35 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "17/12/14 10:25:35 INFO Utils: Successfully started service 'SparkUI' on port 4043.\n",
      "17/12/14 10:25:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.105:4043\n",
      "17/12/14 10:25:36 INFO SparkContext: Added file file:/home/joao/2016-2017/Spark_aulas/words_frequency.py at spark://192.168.1.105:39487/files/words_frequency.py with timestamp 1513247136232\n",
      "17/12/14 10:25:36 INFO Utils: Copying /home/joao/2016-2017/Spark_aulas/words_frequency.py to /tmp/spark-e47768cb-9120-4be0-8c0f-7ff720526b91/userFiles-9640de95-baf9-49b3-bed2-3d159c867550/words_frequency.py\n",
      "17/12/14 10:25:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://192.168.1.105:7077...\n",
      "17/12/14 10:25:36 INFO TransportClientFactory: Successfully created connection to /192.168.1.105:7077 after 39 ms (0 ms spent in bootstraps)\n",
      "17/12/14 10:25:36 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20171214102536-0007\n",
      "17/12/14 10:25:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44020.\n",
      "17/12/14 10:25:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20171214102536-0007/0 on worker-20171206221311-192.168.1.102-33563 (192.168.1.102:33563) with 4 cores\n",
      "17/12/14 10:25:36 INFO StandaloneSchedulerBackend: Granted executor ID app-20171214102536-0007/0 on hostPort 192.168.1.102:33563 with 4 cores, 1024.0 MB RAM\n",
      "17/12/14 10:25:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20171214102536-0007/1 on worker-20171206221310-192.168.1.101-46183 (192.168.1.101:46183) with 4 cores\n",
      "17/12/14 10:25:36 INFO StandaloneSchedulerBackend: Granted executor ID app-20171214102536-0007/1 on hostPort 192.168.1.101:46183 with 4 cores, 1024.0 MB RAM\n",
      "17/12/14 10:25:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20171214102536-0007/2 on worker-20171206221310-192.168.1.100-36805 (192.168.1.100:36805) with 4 cores\n",
      "17/12/14 10:25:36 INFO StandaloneSchedulerBackend: Granted executor ID app-20171214102536-0007/2 on hostPort 192.168.1.100:36805 with 4 cores, 1024.0 MB RAM\n",
      "17/12/14 10:25:36 INFO NettyBlockTransferService: Server created on 192.168.1.105:44020\n",
      "17/12/14 10:25:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "17/12/14 10:25:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.105, 44020, None)\n",
      "17/12/14 10:25:36 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.105:44020 with 3.0 GB RAM, BlockManagerId(driver, 192.168.1.105, 44020, None)\n",
      "17/12/14 10:25:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.105, 44020, None)\n",
      "17/12/14 10:25:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.105, 44020, None)\n",
      "17/12/14 10:25:36 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20171214102536-0007/1 is now RUNNING\n",
      "17/12/14 10:25:36 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20171214102536-0007/2 is now RUNNING\n",
      "17/12/14 10:25:36 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20171214102536-0007/0 is now RUNNING\n",
      "17/12/14 10:25:36 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "17/12/14 10:25:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 236.5 KB, free 3.0 GB)\n",
      "17/12/14 10:25:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.9 KB, free 3.0 GB)\n",
      "17/12/14 10:25:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.105:44020 (size: 22.9 KB, free: 3.0 GB)\n",
      "17/12/14 10:25:37 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "17/12/14 10:25:37 INFO FileInputFormat: Total input paths to process : 1\n",
      "17/12/14 10:25:38 INFO SparkContext: Starting job: count at /home/joao/2016-2017/Spark_aulas/words_frequency.py:12\n",
      "17/12/14 10:25:38 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /home/joao/2016-2017/Spark_aulas/words_frequency.py:11)\n",
      "17/12/14 10:25:38 INFO DAGScheduler: Got job 0 (count at /home/joao/2016-2017/Spark_aulas/words_frequency.py:12) with 4 output partitions\n",
      "17/12/14 10:25:38 INFO DAGScheduler: Final stage: ResultStage 1 (count at /home/joao/2016-2017/Spark_aulas/words_frequency.py:12)\n",
      "17/12/14 10:25:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "17/12/14 10:25:38 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "17/12/14 10:25:38 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /home/joao/2016-2017/Spark_aulas/words_frequency.py:11), which has no missing parents\n",
      "17/12/14 10:25:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.4 KB, free 3.0 GB)\n",
      "17/12/14 10:25:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.1 KB, free 3.0 GB)\n",
      "17/12/14 10:25:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.105:44020 (size: 6.1 KB, free: 3.0 GB)\n",
      "17/12/14 10:25:38 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:996\n",
      "17/12/14 10:25:38 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /home/joao/2016-2017/Spark_aulas/words_frequency.py:11)\n",
      "17/12/14 10:25:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks\n",
      "17/12/14 10:25:38 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(null) (192.168.1.101:59738) with ID 1\n",
      "17/12/14 10:25:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.101, executor 1, partition 0, ANY, 6134 bytes)\n",
      "17/12/14 10:25:38 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 192.168.1.101, executor 1, partition 1, ANY, 6134 bytes)\n",
      "17/12/14 10:25:38 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, 192.168.1.101, executor 1, partition 2, ANY, 6134 bytes)\n",
      "17/12/14 10:25:38 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, 192.168.1.101, executor 1, partition 3, ANY, 6134 bytes)\n",
      "17/12/14 10:25:38 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(null) (192.168.1.102:38160) with ID 0\n",
      "17/12/14 10:25:38 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(null) (192.168.1.100:51870) with ID 2\n",
      "17/12/14 10:25:38 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.101:46705 with 366.3 MB RAM, BlockManagerId(1, 192.168.1.101, 46705, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/12/14 10:25:39 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.100:33820 with 366.3 MB RAM, BlockManagerId(2, 192.168.1.100, 33820, None)\n",
      "17/12/14 10:25:39 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.102:33729 with 366.3 MB RAM, BlockManagerId(0, 192.168.1.102, 33729, None)\n",
      "17/12/14 10:25:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.101:46705 (size: 6.1 KB, free: 366.3 MB)\n",
      "17/12/14 10:25:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.101:46705 (size: 22.9 KB, free: 366.3 MB)\n",
      "17/12/14 10:26:11 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 32595 ms on 192.168.1.101 (executor 1) (1/4)\n",
      "17/12/14 10:26:14 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 35557 ms on 192.168.1.101 (executor 1) (2/4)\n",
      "17/12/14 10:26:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 37317 ms on 192.168.1.101 (executor 1) (3/4)\n",
      "17/12/14 10:26:16 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 37654 ms on 192.168.1.101 (executor 1) (4/4)\n",
      "17/12/14 10:26:16 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /home/joao/2016-2017/Spark_aulas/words_frequency.py:11) finished in 38.369 s\n",
      "17/12/14 10:26:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "17/12/14 10:26:16 INFO DAGScheduler: looking for newly runnable stages\n",
      "17/12/14 10:26:16 INFO DAGScheduler: running: Set()\n",
      "17/12/14 10:26:16 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
      "17/12/14 10:26:16 INFO DAGScheduler: failed: Set()\n",
      "17/12/14 10:26:16 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at count at /home/joao/2016-2017/Spark_aulas/words_frequency.py:12), which has no missing parents\n",
      "17/12/14 10:26:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.2 KB, free 3.0 GB)\n",
      "17/12/14 10:26:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.7 KB, free 3.0 GB)\n",
      "17/12/14 10:26:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.105:44020 (size: 4.7 KB, free: 3.0 GB)\n",
      "17/12/14 10:26:16 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996\n",
      "17/12/14 10:26:16 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (PythonRDD[6] at count at /home/joao/2016-2017/Spark_aulas/words_frequency.py:12)\n",
      "17/12/14 10:26:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks\n",
      "17/12/14 10:26:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, 192.168.1.101, executor 1, partition 0, NODE_LOCAL, 5899 bytes)\n",
      "17/12/14 10:26:16 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, 192.168.1.101, executor 1, partition 1, NODE_LOCAL, 5899 bytes)\n",
      "17/12/14 10:26:16 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, 192.168.1.101, executor 1, partition 2, NODE_LOCAL, 5899 bytes)\n",
      "17/12/14 10:26:16 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, 192.168.1.101, executor 1, partition 3, NODE_LOCAL, 5899 bytes)\n",
      "17/12/14 10:26:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.101:46705 (size: 4.7 KB, free: 366.3 MB)\n",
      "17/12/14 10:26:16 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.101:59738\n",
      "17/12/14 10:26:16 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 162 bytes\n",
      "17/12/14 10:26:18 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 1647 ms on 192.168.1.101 (executor 1) (1/4)\n",
      "17/12/14 10:26:18 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 1659 ms on 192.168.1.101 (executor 1) (2/4)\n",
      "17/12/14 10:26:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 1678 ms on 192.168.1.101 (executor 1) (3/4)\n",
      "17/12/14 10:26:18 INFO DAGScheduler: ResultStage 1 (count at /home/joao/2016-2017/Spark_aulas/words_frequency.py:12) finished in 1.699 s\n",
      "17/12/14 10:26:18 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 1693 ms on 192.168.1.101 (executor 1) (4/4)\n",
      "17/12/14 10:26:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "17/12/14 10:26:18 INFO DAGScheduler: Job 0 finished: count at /home/joao/2016-2017/Spark_aulas/words_frequency.py:12, took 40.215237 s\n",
      "\u001b]0;IPython: 2016-2017/Spark_aulas\u0007Número de palavras no ficheiro: 2482193\n",
      "O cálculo demorou 41.53s.\n",
      "17/12/14 10:26:18 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "17/12/14 10:26:18 INFO SparkUI: Stopped Spark web UI at http://192.168.1.105:4043\n",
      "17/12/14 10:26:18 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "17/12/14 10:26:18 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "17/12/14 10:26:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "17/12/14 10:26:18 INFO MemoryStore: MemoryStore cleared\n",
      "17/12/14 10:26:18 INFO BlockManager: BlockManager stopped\n",
      "17/12/14 10:26:18 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "17/12/14 10:26:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "17/12/14 10:26:18 INFO SparkContext: Successfully stopped SparkContext\n",
      "17/12/14 10:26:18 INFO ShutdownHookManager: Shutdown hook called\n",
      "17/12/14 10:26:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-e47768cb-9120-4be0-8c0f-7ff720526b91\n",
      "17/12/14 10:26:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-e47768cb-9120-4be0-8c0f-7ff720526b91/pyspark-92662a9e-b031-49db-8fe3-9cdd85b5adef\n"
     ]
    }
   ],
   "source": [
    "!/usr/share/spark/bin/spark-submit \\\n",
    "--master spark://192.168.1.105:7077 \\\n",
    "words_frequency.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opcional: gravar para ficheiro (local ou hdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts.saveAsTextFile(\"hdfs://...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
