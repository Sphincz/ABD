{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering de Filmes baseado no resumo dos filmes (synopsis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descrição do trabalho\n",
    "\n",
    "Neste _notebook_ vamos fazer _clustering_ de filmes baseado na descrição textual (_synopsis_) retirada da base de dados de filmes [IMDb](http://www.imdb.com/).\n",
    "\n",
    "A similaridade entre filmes será calculada a partir da medida TF-IDF (_Term Frequency - Inverse Document Frequency_) do conteúdo dos resumos dos filmes. Atenção que há vários factores que contribuem para a correcção dos resultados, pelo que o que se pretende neste notebook é exemplificar a metodologia :-)\n",
    "\n",
    "Resumo das operações realizadas:\n",
    "\n",
    "1. [Scraping dos 100 melhores filmes](#1.&nbsp;Scraping&nbsp;dos&nbsp;100&nbsp;melhores&nbsp;filmes)\n",
    "<br>Obter informação dos 100 melhores filmes de todos os tempos (de acordo com este [utilizador](http://www.imdb.com/list/ls055592025/))<br><br>\n",
    "2. [Tokenizing e stemming](#2.&nbsp;Tokenizing&nbsp;e&nbsp;stemming)\n",
    "<br> Criar funções que têm como objectivo transformar um documento num conjunto de palavras (um pouco à semelhança do _shingling_). <br><br>\n",
    "3. [Transformar os dados (tokens) para um espaço vectorial](#3.&nbsp;Transformar&nbsp;os&nbsp;dados&nbsp;para&nbsp;um&nbsp;espaço&nbsp;vectorial) usando a [tf-idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "<br> Após ter um documento representado por um conjunto de palavras, vamos transformar esta representação num vector numérico usando a medida TF-IDF <br><br>\n",
    "4. [Calcular a _cosine distance_ dos documentos](#4.&nbsp;Calcular&nbsp;a&nbsp;cosine&nbsp;distance) \n",
    "<br>Calcular a distância entre todos os documentos usado a TF-IDF e a distância do coseno.<br><br>\n",
    "5. [Clustering dos documentos](#5.&nbsp;K-means&nbsp;clustering) \n",
    "<br>Agrupar os documentos em conjuntos (clusters) usando o algoritmo [k-means](http://en.wikipedia.org/wiki/K-means_clustering)<br><br>\n",
    "6. [Redução de dimensionalidade](#6.&nbsp;Redução&nbsp;de&nbsp;dimensionalidade) \n",
    "<br> Reduzir a dimensionalidade dos dados usando o algoritmo PCA para posterior visualização.\n",
    "<br><br>\n",
    "7. [Gráfico dos clusters](#7.&nbsp;Gráfico&nbsp;dos&nbsp;Clusters) \n",
    "<br> Visualizar os dados dos clusters usando o [matplotlib](http://matplotlib.org/)\n",
    "<br><br>\n",
    "8. [Clustering hierárquico dos filmes](#8.&nbsp;Clustering&nbsp;hierárquico&nbsp;dos&nbsp;filmes) \n",
    "<br>Aplicar o algoritmo de _clustering_ hierárquico [Ward clustering](http://en.wikipedia.org/wiki/Ward%27s_method) aos dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3 \n",
    "import re\n",
    "import codecs\n",
    "import os, shutil\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.&nbsp;Scraping&nbsp;dos&nbsp;100&nbsp;melhores&nbsp;filmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download da lista de filmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "r = http.request('GET', \"http://www.imdb.com/list/ls055592025/\")\n",
    "\n",
    "soup = BeautifulSoup(r.data, \"html.parser\")\n",
    "\n",
    "links = []\n",
    "titles = []\n",
    "\n",
    "for i, div in enumerate(soup.find_all('div', {'class': 'info'})):\n",
    "    for b in div.find_all('b'):\n",
    "        for a in b.find_all('a'):\n",
    "            titles.append(a.text)\n",
    "            links.append(a['href'])\n",
    "            print(i+1, a.text)\n",
    "            \n",
    "print(\"\\nTotal: \" + str(len(links)) + ' links e ' + str(len(titles)) + ' títulos de filmes.')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar os dados na directoria synopses\n",
    "\n",
    "Criar a directoria `synopses`. Se a directoria já existe, apagar tudo e criar de novo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists('synopses'):\n",
    "    shutil.rmtree('synopses')\n",
    "os.mkdir('synopses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazer o download das synopses\n",
    "\n",
    "Fazer o download das _synopses_ dos filmes e guardar em ficheiros dentro da directoria `synopses`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(titles)):\n",
    "    \n",
    "    r2 = http.request('GET', \"http://www.imdb.com\"+links[i])\n",
    "    soup2 = BeautifulSoup(r2.data, \"html.parser\")\n",
    "    \n",
    "    divitem = soup2.find('div', {'itemprop': 'genre'})\n",
    "    genres = []\n",
    "    for a in divitem.find_all('a'):\n",
    "        genres.append(a.text.strip())\n",
    "    \n",
    "    \n",
    "    r3 = http.request('GET', \"http://www.imdb.com\"+links[i]+'plotsummary#synopsis')\n",
    "    soup3 = BeautifulSoup(r3.data, \"html.parser\")\n",
    "    synopsis = soup3.find_all('ul', {'id': 'plot-synopsis-content'})[0].text\n",
    "    \n",
    "    \n",
    "    print('Filme %d: ' %i, titles[i])\n",
    "    print('Géneros: ', genres)\n",
    "    print(synopsis[:300].strip()+'[...]'+'\\n')\n",
    "    if not \"It looks like we don't have a Synopsis for this title yet\" in synopsis[:80].strip():\n",
    "        with open('synopses/movie_%02d.txt' % i, 'w') as f:\n",
    "            f.write(synopsis)\n",
    "    else:\n",
    "        with open('synopses/movie_%02d.txt' % i, 'w') as f:\n",
    "            f.write('n.a.')\n",
    "\n",
    "    with open('synopses/titles.txt', 'a') as f:\n",
    "        f.write(titles[i] + '\\n')\n",
    "        \n",
    "    with open('synopses/genres.txt', 'a') as f:\n",
    "        f.write(str(genres) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.&nbsp;Tokenizing&nbsp;e&nbsp;stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar os dados\n",
    "\n",
    "Vamos importar os títulos, os links das páginas, os resumos e os géneros dos filmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filmes = open('synopses/titles.txt').read().strip().split('\\n')\n",
    "genres = open('synopses/genres.txt').read().strip().split('\\n')\n",
    "ranks = [i for i in range(0,len(filmes))]\n",
    "\n",
    "synopses = []\n",
    "erased = 0\n",
    "for i in range(len(filmes)):\n",
    "    texto = open('synopses/movie_%02d.txt' % i).read().strip('\\n')\n",
    "    if not texto == 'n.a.':\n",
    "        synopses.append(texto)\n",
    "    else:\n",
    "        del ranks[i-erased]\n",
    "        del genres[i-erased]\n",
    "        del filmes[i-erased]\n",
    "        erased +=1\n",
    "\n",
    "print(str(len(titles)) + ' filmes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_filme = 10\n",
    "print('Título do filme:', filmes[id_filme])\n",
    "print('Género:', genres[id_filme])\n",
    "print('Resumo:', synopses[id_filme])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obter uma representação dos resumos dos filmes, vamos considerar a frequência das próprias palavras. Para isso é preciso:\n",
    "\n",
    "- ignorar as _stop words_\n",
    "- realizar a operação _tokeninzing_, isto é, separar o texto em palavras\n",
    "- realizar a operação _stemming_, que consiste em representar qualquer palavra pela sua versão _stemmed_, isto, a base que esteve na origem da palavra. Por exemplo, 'runs', 'running' vão ser todas transformadas em 'run'. A ideia é que todas \"transmitem\" a mesma ideia: correr!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar as operações atrás descritas, vamos usar a biblioteca [NLTK's](http://www.nltk.org/).\n",
    "Primeiro vamos importar a lista de [_stop words_](http://en.wikipedia.org/wiki/Stop_words). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida vamos importar o [Snowball Stemmer](http://snowball.tartarus.org/) que faz parte do NLTK.\n",
    "ou o [Porter Stemmer](http://snowball.tartarus.org/algorithms/porter/stemmer.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nltk.stem.porter import PorterStemmer\n",
    "# stemmer = PorterStemmer()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir duas funções:\n",
    "\n",
    "- *tokenize_and_stem*: _tokenizes_ (separa o texto em palavras - _tokens_) e realiza a operação _stemming_ a cada palavra (token)\n",
    "- *tokenize_only*: separa apenas o texto em _tokens_.\n",
    "\n",
    "Ambas as funções serão usadas mais tarde, uma para criar um dicionário de _stems_ e outra para obter a palavra original. (isto será importante na parte de análise).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_e_stem(texto):\n",
    "    # A tarefa de obter tokens vai ser realizada em duas fases: primeiro frases e depois palavras.\n",
    "    tokens = [palavra for frase in nltk.sent_tokenize(texto) for palavra in nltk.word_tokenize(frase)]\n",
    "\n",
    "    # filtrar tokens que não tenham letras (exemplo: valores numéricos, só pontuação, etc...)\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.&nbsp;Transformar&nbsp;os&nbsp;dados&nbsp;para&nbsp;um&nbsp;espaço&nbsp;vectorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicar as funções aos resumos dos filmes (synopses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for doc_i in synopses:\n",
    "    allwords_stemmed = tokenize_e_stem(doc_i)\n",
    "    totalvocab_stemmed.extend(allwords_stemmed)\n",
    "    \n",
    "    allwords_tokenized = tokenize(doc_i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relação entre palavras e versão stemmed\n",
    "\n",
    "Usando estas duas listas, vamos criar um `DataFrame` do `python pandas` com o vocabulário _stemmed_ como índice e os tokens como palavras. A ideia é ter uma forma eficiente de obter a versão completa da palavra a partir da sua versão _stemmed_. O único senão é que não será possível recuperar a palavra original, porque não temos uma relação de um para um. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF and document similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://www.jiem.org/index.php/jiem/article/viewFile/293/252/2402' align='right' style=\"margin-left:10px\">\n",
    "\n",
    "\n",
    "Para comparar os diferentes filmes, vamos usar a TF-IDF (_term frequency - inverse document frequency_). A ideia é que uma palavra que apareça muitas vezes num documento é importante. Por outro lado, essa palavra é tão mais importante se aparecer poucas vezes nos documentos todos.\n",
    "\n",
    "Para obter uma matriz Tf-idf primeiro é necessário contar as occurrências por documento. Depois transforma-se numa matriz _document-term matrix_ (dtm) que está representada à direita na figura. Esta matriz também é conhecida como a _term frequency matrix_.\n",
    "\n",
    "Algumas notas da implementação:\n",
    "\n",
    "- `max_df`: valor máximo para a frequência de uma _feature_ dentro de um documento para ser usada na matriz tf-idf. A ideia é que um termo existe em mais de 80% dos documentos, provavelmente não tem grande significado (no contexto dos filmes).\n",
    "- `min_idf`: valor mínimo com que um _feature_ tem de aparecer na globalidade dos documentos.\n",
    "- `ngram_range`: qual o tipo de [n-grams](http://en.wikipedia.org/wiki/N-gram) que vamos considerar: unigramas (palavras simples), bigramas (duas palavras de seguida) e trigramas (três palavras de seguida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8,\n",
    "                                   max_features=200000,\n",
    "                                   min_df=0.2, \n",
    "                                   stop_words='english',\n",
    "                                   use_idf=True,\n",
    "                                   tokenizer=tokenize_e_stem,\n",
    "                                   ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(synopses)\n",
    "\n",
    "print('A matris TD-IDF tem dimensões:', tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ver a lista dos primeiros \"tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "print('Existem ', len(terms), 'termos usados.')\n",
    "print(terms[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.&nbsp;Calcular&nbsp;a&nbsp;cosine&nbsp;distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity <-- o import já foi feito no início do documento\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A matriz de distâncias tem dimensão:', dist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5.&nbsp;K-means&nbsp;clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupar os dados em clusters\n",
    "\n",
    "Para melhor compreender a estrutura dos dados, nomeadamente dos resumos dos filmes, vamos usar o algoritmo [k-means](http://en.wikipedia.org/wiki/K-means_clustering) usando 5 clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters);\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "# passar a classificação para uma lista\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar um `DataFrame` com os títulos dos filmes, o rank do filme na lista, o cluster e o género de filme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "films = { 'titulo': filmes, 'rank': ranks, 'synopsis': synopses, 'cluster': clusters, 'Género': genres }\n",
    "frame = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'titulo', 'cluster', 'Género'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frame\n",
    "frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algumas estatísticas\n",
    "### Número de filmes por cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valor médio do rank por cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = frame['rank'].groupby(frame['cluster'])\n",
    "grouped.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top \"palavras\" por cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos mostrar, para cada cluster, quais as palavras que definem o cluster e que filmes se encontram em cada cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_words =[[] for i in range(num_clusters)]\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(num_clusters):\n",
    "    #print(\"-\" * 80)\n",
    "    print(\"Palavras do cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :6]:\n",
    "        print(' %s,' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0], end='')\n",
    "        if len(cluster_words[i])<3:\n",
    "            cluster_words[i].append(vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0])\n",
    "    print(\"\\nFilmes do cluster %d:\" % i)\n",
    "    for title in frame.ix[i]['titulo'].values.tolist():\n",
    "        print('   - %s' % title)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.&nbsp;Redução&nbsp;de&nbsp;dimensionalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para representar os filmes, vamos reduzir a dimensionalidade do problema, transformado a matriz de distâncias num array bi-dimensional usando o algoritmo PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "#pos = pca.fit(X).transform(X)\n",
    "pos = pca.fit_transform(dist)\n",
    "xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.&nbsp;Gráfico&nbsp;dos&nbsp;Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir o nome e as cores do cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definir a cor dos clusters\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n",
    "\n",
    "# Definir o nome dos cluster (usando um dicionário)\n",
    "cluster_names = { i: cluster_words[i] for i in range(num_clusters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizar os dados num DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cirar um dataframse que seja o resultado do PCA, o número dos clusters e o título dos filmes\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=filmes)) \n",
    "\n",
    "# agrupar por cluster\n",
    "groups = df.groupby('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrar o gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',         # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='off',        # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelleft='off')\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=8)  \n",
    "\n",
    "    \n",
    "    \n",
    "plt.show() #show the plot\n",
    "\n",
    "#A linha seguinte permite gravar o gráfico como uma imagem\n",
    "#plt.savefig('cluster_filmes.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.&nbsp;Clustering&nbsp;hierárquico&nbsp;dos&nbsp;filmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora usar _hierarchical clustering_ para representar a mesma informação. O algoritmo usado neste exemplo é o [Ward clustering algorithm](http://en.wikipedia.org/wiki/Ward%27s_method). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 30)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=filmes, leaf_font_size=15);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "\n",
    "#para gravar a figura:\n",
    "#plt.savefig('cluster_filmes_hiearquico.png', dpi=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
